{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('US_nn_plus.csv')\n",
    "#Combine the last three columns into one simple string column\n",
    "data['text'] = data['movie'] + ' ' + data['synopsis'] + ' ' + data['taglines']\n",
    "data.drop(['movie', 'synopsis', 'taglines'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\project2\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('F:\\Bert')\n",
    "text_input = data['text']\n",
    "text_input = text_input.fillna('').apply(lambda x: tokenizer.encode(x, add_special_tokens=False))\n",
    "#Make sure all sequences have the same length 512\n",
    "text_input = [i[:512] for i in text_input]\n",
    "attention_mask = [[1] * len(i) + [0] * (512 - len(i)) for i in text_input]\n",
    "text_input = [i + [0] * (512 - len(i)) for i in text_input]\n",
    "bert = BertModel.from_pretrained('F:\\Bert').to('cuda')\n",
    "text_input = torch.tensor(text_input).to('cuda')\n",
    "attention_mask = torch.tensor(attention_mask).to('cuda')\n",
    "bert(text_input[0].unsqueeze(0))\n",
    "output = torch.zeros((len(text_input), 512, 1024)).to('cuda')\n",
    "bert.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(text_input) // 64 + 1):\n",
    "        torch.cuda.empty_cache()\n",
    "        output[i:i+64]  = bert(text_input[i:i+64], attention_mask=attention_mask[i:i+64]).last_hidden_state\n",
    "        print('Current progress:', i / (len(text_input) // 64 + 1))\n",
    "del bert\n",
    "del text_input\n",
    "del attention_mask\n",
    "output.to('cpu')\n",
    "text_input = output\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "torch.save(text_input, 'text_input.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('US_nn_plus.csv')\n",
    "#Combine the last three columns into one simple string column\n",
    "data['text'] = data['movie'] + ' ' + data['synopsis'] + ' ' + data['taglines']\n",
    "data.drop(['movie', 'synopsis', 'taglines'], axis=1, inplace=True)\n",
    "text_input = torch.load('text_input.pt')\n",
    "profit_mean = data['profit'].mean()\n",
    "profit_std = data['profit'].std()\n",
    "def encode_profit(x):\n",
    "    return (x - profit_mean) / profit_std\n",
    "def decode_profit(x):\n",
    "    return x * profit_std + profit_mean\n",
    "data['profit'] = data['profit'].apply(encode_profit)\n",
    "text_input = text_input.float()\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, text_input):\n",
    "        self.input = torch.tensor(data[data.columns[4:-1]].values, dtype=torch.float32)\n",
    "        self.text_input = text_input[:len(self.input)]\n",
    "        self.output = torch.tensor(data[data.columns[0:4]].values, dtype=torch.float32)\n",
    "    def __getitem__(self, index):\n",
    "        x = self.input[index]\n",
    "        x_text = self.text_input[index]\n",
    "        y_profits = self.output[index, 0]\n",
    "        y_recom = self.output[index, 1:]\n",
    "        return x, x_text, y_profits, y_recom\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "dataset = CustomDataset(data,text_input)\n",
    "train, val = torch.utils.data.random_split(dataset, [0.8,0.2])\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=len(val), shuffle=True)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,drop=0.3):\n",
    "        super(Net, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(1024, 1,dropout=drop)\n",
    "        self.fc_for_text = nn.Linear(1024*512, 16)\n",
    "        self.fc = nn.Linear(13, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.dropout2 = nn.Dropout(drop)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "    def forward (self, x, x_text):\n",
    "        x_text = self.attention(x_text, x_text, x_text)[0]\n",
    "        x_text = self.fc_for_text(x_text.view(x_text.shape[0], -1))\n",
    "        x = self.fc(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(torch.cat([x, x_text], axis=1)))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "def loss_fn(x, y_profits, y_recom):\n",
    "    profits = x[:,0]\n",
    "    recom = F.softmax(x[:,1:], dim=1)\n",
    "    return F.binary_cross_entropy(recom, y_recom)\n",
    "model = Net(0.5).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.05)\n",
    "#Use simple scheduler\n",
    "sceduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "def train_one_epoch(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for x, x_text, y_profits, y_recom in train_loader:\n",
    "        x = x.to('cuda')\n",
    "        x_text = x_text.to('cuda')\n",
    "        y_profits = y_profits.to('cuda')\n",
    "        y_recom = y_recom.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, x_text)\n",
    "        loss = loss_fn(output, y_profits, y_recom)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    return loss.item()/len(train_loader)\n",
    "def val_loss(model, val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, x_text, y_profits, y_recom in val:\n",
    "            x = x.to('cuda')\n",
    "            x_text = x_text.to('cuda')\n",
    "            y_profits = y_profits.to('cuda')\n",
    "            y_recom = y_recom.to('cuda')\n",
    "            output = model(x, x_text)\n",
    "            loss = loss_fn(output, y_profits, y_recom)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.05221553643544515\n",
      "Validation loss: 48.779212951660156\n",
      "Epoch: 1\n",
      "Train loss: 0.04620980421702067\n",
      "Validation loss: 33.96809005737305\n",
      "Epoch: 2\n",
      "Train loss: 0.04406023820241292\n",
      "Validation loss: 7.5883049964904785\n",
      "Epoch: 3\n",
      "Train loss: 0.043439420064290364\n",
      "Validation loss: 33.95083236694336\n",
      "Epoch: 4\n",
      "Train loss: 0.04233328501383463\n",
      "Validation loss: 36.65122604370117\n",
      "Epoch: 5\n",
      "Train loss: 0.044561461607615156\n",
      "Validation loss: 36.9450798034668\n",
      "Epoch: 6\n",
      "Train loss: 0.04455483357111613\n",
      "Validation loss: 37.90485382080078\n",
      "Epoch: 7\n",
      "Train loss: 0.03754165569941203\n",
      "Validation loss: 40.11179733276367\n",
      "Epoch: 8\n",
      "Train loss: 0.03922600746154785\n",
      "Validation loss: 45.906349182128906\n",
      "Epoch: 9\n",
      "Train loss: 0.03644063472747803\n",
      "Validation loss: 51.04633331298828\n",
      "Epoch: 10\n",
      "Train loss: 0.03459502061208089\n",
      "Validation loss: 56.226009368896484\n",
      "Epoch: 11\n",
      "Train loss: 0.03842264413833618\n",
      "Validation loss: 26.189849853515625\n",
      "Epoch: 12\n",
      "Train loss: 0.03296025196711223\n",
      "Validation loss: 19.115825653076172\n",
      "Epoch: 13\n",
      "Train loss: 0.038953510920206706\n",
      "Validation loss: 6.5748701095581055\n",
      "Epoch: 14\n",
      "Train loss: 0.034515019257863364\n",
      "Validation loss: 24.275836944580078\n",
      "Epoch: 15\n",
      "Train loss: 0.03183931310971578\n",
      "Validation loss: 24.774616241455078\n",
      "Epoch: 16\n",
      "Train loss: 0.028805102904637656\n",
      "Validation loss: 13.643614768981934\n",
      "Epoch: 17\n",
      "Train loss: 0.03456239302953084\n",
      "Validation loss: 2.9164907932281494\n",
      "Epoch: 18\n",
      "Train loss: 0.028588497638702394\n",
      "Validation loss: 7.599349021911621\n",
      "Epoch: 19\n",
      "Train loss: 0.028377850850423176\n",
      "Validation loss: 25.61536979675293\n",
      "Epoch: 20\n",
      "Train loss: 0.03139312068621317\n",
      "Validation loss: 3.0200347900390625\n",
      "Epoch: 21\n",
      "Train loss: 0.03016550342241923\n",
      "Validation loss: 54.19126510620117\n",
      "Epoch: 22\n",
      "Train loss: 0.028328577677408855\n",
      "Validation loss: 11.443499565124512\n",
      "Epoch: 23\n",
      "Train loss: 0.03075317939122518\n",
      "Validation loss: 12.294625282287598\n",
      "Epoch: 24\n",
      "Train loss: 0.03112969398498535\n",
      "Validation loss: 9.789644241333008\n",
      "Epoch: 25\n",
      "Train loss: 0.028877878189086915\n",
      "Validation loss: 27.71029281616211\n",
      "Epoch: 26\n",
      "Train loss: 0.02590234676996867\n",
      "Validation loss: 11.954511642456055\n",
      "Epoch: 27\n",
      "Train loss: 0.029422962665557863\n",
      "Validation loss: 18.071949005126953\n",
      "Epoch: 28\n",
      "Train loss: 0.029839340845743814\n",
      "Validation loss: 7.721290588378906\n",
      "Epoch: 29\n",
      "Train loss: 0.02990321715672811\n",
      "Validation loss: 17.91680145263672\n",
      "Epoch: 30\n",
      "Train loss: 0.02445253133773804\n",
      "Validation loss: 13.585894584655762\n",
      "Epoch: 31\n",
      "Train loss: 0.02786770462989807\n",
      "Validation loss: 4.477405071258545\n",
      "Epoch: 32\n",
      "Train loss: 0.03114370902379354\n",
      "Validation loss: 13.885237693786621\n",
      "Epoch: 33\n",
      "Train loss: 0.027952253818511963\n",
      "Validation loss: 22.68194580078125\n",
      "Epoch: 34\n",
      "Train loss: 0.02834106683731079\n",
      "Validation loss: 9.268609046936035\n",
      "Epoch: 35\n",
      "Train loss: 0.027785672744115194\n",
      "Validation loss: 13.698859214782715\n",
      "Epoch: 36\n",
      "Train loss: 0.02417541742324829\n",
      "Validation loss: 21.610946655273438\n",
      "Epoch: 37\n",
      "Train loss: 0.025713735818862916\n",
      "Validation loss: 1.0206165313720703\n",
      "Epoch: 38\n",
      "Train loss: 0.02122811277707418\n",
      "Validation loss: 14.72930908203125\n",
      "Epoch: 39\n",
      "Train loss: 0.027570945024490357\n",
      "Validation loss: 10.668807029724121\n",
      "Epoch: 40\n",
      "Train loss: 0.025020329157511394\n",
      "Validation loss: 54.3116455078125\n",
      "Epoch: 41\n",
      "Train loss: 0.02765246828397115\n",
      "Validation loss: 24.18270492553711\n",
      "Epoch: 42\n",
      "Train loss: 0.021770334243774413\n",
      "Validation loss: 10.998790740966797\n",
      "Epoch: 43\n",
      "Train loss: 0.03031201958656311\n",
      "Validation loss: 22.36269187927246\n",
      "Epoch: 44\n",
      "Train loss: 0.030392549435297646\n",
      "Validation loss: 16.767234802246094\n",
      "Epoch: 45\n",
      "Train loss: 0.02234956423441569\n",
      "Validation loss: 29.104854583740234\n",
      "Epoch: 46\n",
      "Train loss: 0.028951879342397055\n",
      "Validation loss: 16.434011459350586\n",
      "Epoch: 47\n",
      "Train loss: 0.027548287312189737\n",
      "Validation loss: 4.799049377441406\n",
      "Epoch: 48\n",
      "Train loss: 0.021650211016337077\n",
      "Validation loss: 54.255680084228516\n",
      "Epoch: 49\n",
      "Train loss: 0.02840160330136617\n",
      "Validation loss: 12.994634628295898\n",
      "Epoch: 50\n",
      "Train loss: 0.02934406598409017\n",
      "Validation loss: 0.6719633340835571\n",
      "Epoch: 51\n",
      "Train loss: 0.03028924862543742\n",
      "Validation loss: 2.147503137588501\n",
      "Epoch: 52\n",
      "Train loss: 0.029381990432739258\n",
      "Validation loss: 12.719893455505371\n",
      "Epoch: 53\n",
      "Train loss: 0.028684008121490478\n",
      "Validation loss: 19.292633056640625\n",
      "Epoch: 54\n",
      "Train loss: 0.025525734821955363\n",
      "Validation loss: 2.917527437210083\n",
      "Epoch: 55\n",
      "Train loss: 0.021911458174387614\n",
      "Validation loss: 20.86549186706543\n",
      "Epoch: 56\n",
      "Train loss: 0.02767829696337382\n",
      "Validation loss: 5.2977142333984375\n",
      "Epoch: 57\n",
      "Train loss: 0.023224709431330363\n",
      "Validation loss: 13.930173873901367\n",
      "Epoch: 58\n",
      "Train loss: 0.02649176518122355\n",
      "Validation loss: 0.5102663636207581\n",
      "Epoch: 59\n",
      "Train loss: 0.017110339800516763\n",
      "Validation loss: 15.028656959533691\n",
      "Epoch: 60\n",
      "Train loss: 0.025411959489186606\n",
      "Validation loss: 14.21994400024414\n",
      "Epoch: 61\n",
      "Train loss: 0.027772251764933267\n",
      "Validation loss: 54.289302825927734\n",
      "Epoch: 62\n",
      "Train loss: 0.022951577107111612\n",
      "Validation loss: 14.681246757507324\n",
      "Epoch: 63\n",
      "Train loss: 0.03319233258565267\n",
      "Validation loss: 29.106712341308594\n",
      "Epoch: 64\n",
      "Train loss: 0.025263927380243936\n",
      "Validation loss: 26.27758026123047\n",
      "Epoch: 65\n",
      "Train loss: 0.03060545523961385\n",
      "Validation loss: 0.7902988195419312\n",
      "Epoch: 66\n",
      "Train loss: 0.026644782225290934\n",
      "Validation loss: 13.698380470275879\n",
      "Epoch: 67\n",
      "Train loss: 0.01959212025006612\n",
      "Validation loss: 19.232444763183594\n",
      "Epoch: 68\n",
      "Train loss: 0.027884083986282348\n",
      "Validation loss: 27.475309371948242\n",
      "Epoch: 69\n",
      "Train loss: 0.029082765181859333\n",
      "Validation loss: 2.374567985534668\n",
      "Epoch: 70\n",
      "Train loss: 0.026982996861139932\n",
      "Validation loss: 12.457810401916504\n",
      "Epoch: 71\n",
      "Train loss: 0.03057039181391398\n",
      "Validation loss: 16.925600051879883\n",
      "Epoch: 72\n",
      "Train loss: 0.022007485230763752\n",
      "Validation loss: 54.683555603027344\n",
      "Epoch: 73\n",
      "Train loss: 0.028834132353464763\n",
      "Validation loss: 0.9762299060821533\n",
      "Epoch: 74\n",
      "Train loss: 0.029700690507888795\n",
      "Validation loss: 54.20180130004883\n",
      "Epoch: 75\n",
      "Train loss: 0.023183262348175047\n",
      "Validation loss: 15.215386390686035\n",
      "Epoch: 76\n",
      "Train loss: 0.03436036109924316\n",
      "Validation loss: 55.21896743774414\n",
      "Epoch: 77\n",
      "Train loss: 0.02711014747619629\n",
      "Validation loss: 1.9697998762130737\n",
      "Epoch: 78\n",
      "Train loss: 0.02553712526957194\n",
      "Validation loss: 16.351774215698242\n",
      "Epoch: 79\n",
      "Train loss: 0.024207353591918945\n",
      "Validation loss: 18.4586124420166\n",
      "Epoch: 80\n",
      "Train loss: 0.03277616302172343\n",
      "Validation loss: 12.225177764892578\n",
      "Epoch: 81\n",
      "Train loss: 0.027116088072458903\n",
      "Validation loss: 54.847267150878906\n",
      "Epoch: 82\n",
      "Train loss: 0.027354888121287026\n",
      "Validation loss: 23.636144638061523\n",
      "Epoch: 83\n",
      "Train loss: 0.02286046346028646\n",
      "Validation loss: 7.380436420440674\n",
      "Epoch: 84\n",
      "Train loss: 0.02226322889328003\n",
      "Validation loss: 21.97010612487793\n",
      "Epoch: 85\n",
      "Train loss: 0.031197245915730795\n",
      "Validation loss: 12.431180953979492\n",
      "Epoch: 86\n",
      "Train loss: 0.03340371449788412\n",
      "Validation loss: 34.03319549560547\n",
      "Epoch: 87\n",
      "Train loss: 0.025709684689839682\n",
      "Validation loss: 12.056575775146484\n",
      "Epoch: 88\n",
      "Train loss: 0.03075720469156901\n",
      "Validation loss: 11.878349304199219\n",
      "Epoch: 89\n",
      "Train loss: 0.02427267034848531\n",
      "Validation loss: 15.862107276916504\n",
      "Epoch: 90\n",
      "Train loss: 0.03290000557899475\n",
      "Validation loss: 1.7595245838165283\n",
      "Epoch: 91\n",
      "Train loss: 0.025591198603312174\n",
      "Validation loss: 12.835227012634277\n",
      "Epoch: 92\n",
      "Train loss: 0.02814546227455139\n",
      "Validation loss: 0.7895338535308838\n",
      "Epoch: 93\n",
      "Train loss: 0.03724056879679362\n",
      "Validation loss: 0.8312132954597473\n",
      "Epoch: 94\n",
      "Train loss: 0.027750623226165772\n",
      "Validation loss: 3.460545301437378\n",
      "Epoch: 95\n",
      "Train loss: 0.026193809509277344\n",
      "Validation loss: 1.0743943452835083\n",
      "Epoch: 96\n",
      "Train loss: 0.027000844478607178\n",
      "Validation loss: 0.7776988744735718\n",
      "Epoch: 97\n",
      "Train loss: 0.019918779532114666\n",
      "Validation loss: 1.766919493675232\n",
      "Epoch: 98\n",
      "Train loss: 0.020585391918818155\n",
      "Validation loss: 11.994024276733398\n",
      "Epoch: 99\n",
      "Train loss: 0.021780051787694297\n",
      "Validation loss: 3.0448479652404785\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('Epoch:', i)\n",
    "    print('Train loss:', train_one_epoch(model, optimizer, train_loader))\n",
    "    print('Validation loss:', val_loss(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model\":model.to('cpu'), \"parameters\":model.to('cpu').state_dict()}, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model.pth')['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recom_accuracy(x,x_text,y_recom):\n",
    "    recom = F.softmax(model(x, x_text)[:,1:], dim=1)\n",
    "    recom = torch.argmax(recom, dim=1)\n",
    "    y_recom = torch.argmax(y_recom, dim=1)\n",
    "    return torch.sum(recom == y_recom).item() / len(y_recom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation accuracy: 0.7566666666666667\n",
      "Val Recommendation accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "#Caclulate the accuracy of all datasets\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "x_all = torch.tensor(data[data.columns[4:-1]].values, dtype=torch.float32).to('cuda')\n",
    "x_text_all = text_input.to('cuda')\n",
    "y_recommend_all = torch.tensor(data[data.columns[1:4]].values, dtype=torch.float32).to('cuda')\n",
    "acc_sum = 0\n",
    "for i in range(len(x_all)//32 + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    x = x_all[i*32:(i+1)*32]\n",
    "    x_text = x_text_all[i*32:(i+1)*32]\n",
    "    y_recommend = y_recommend_all[i*32:(i+1)*32]\n",
    "    acc_sum += recom_accuracy(x, x_text, y_recommend)*x.shape[0]\n",
    "print('Recommendation accuracy:', acc_sum/len(x_all))\n",
    "#Calulate Val accuracy\n",
    "torch.cuda.empty_cache()\n",
    "for x_val, x_text_val, y_profits_val, y_recommend_val in val_loader:\n",
    "    x_val = x_val.to('cuda')\n",
    "    x_text_val = x_text_val.to('cuda')\n",
    "    y_recommend_val = y_recommend_val.to('cuda')\n",
    "    y_profits_val = y_profits_val.to('cuda')\n",
    "acc_sum_val = 0\n",
    "for i in range(len(x_val)//32 + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    x = x_val[i*32:(i+1)*32]\n",
    "    x_text = x_text_val[i*32:(i+1)*32]\n",
    "    y_recommend = y_recommend_val[i*32:(i+1)*32]\n",
    "    acc_sum_val += recom_accuracy(x, x_text, y_recommend)*x.shape[0]\n",
    "print('Val Recommendation accuracy:', acc_sum_val/len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7566666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_poor'].sum() / len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
